{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas openpyxl\n",
    "!pip install bitsandbytes==0.38.1\n",
    "!pip install torch transformers>=4.28.0 peft==0.4.0 sentencepiece bitsandbytes accelerate\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# --------------- A) 超參數 ---------------\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "MAX_LENGTH = 256\n",
    "OUTPUT_DIR = \"chinese-alpaca-pro-33b-out\"\n",
    "BASE_MODEL = \"minlik/chinese-alpaca-pro-33b-merged\"\n",
    "\n",
    "# --------------- B) Prompt 設計 ---------------\n",
    "def generate_prompt(text):\n",
    "    \"\"\"\n",
    "    通用 Prompt 設計，適用於微調和推理\n",
    "    \"\"\"\n",
    "    return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{text}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "def format_prompt_and_labels(example):\n",
    "    \"\"\"\n",
    "    用於微調的格式化：生成 Prompt 和輸出\n",
    "    \"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output_text = example[\"output\"]\n",
    "\n",
    "    if input_text.strip():\n",
    "        prompt = generate_prompt(f\"{instruction}\\n\\n{input_text}\")\n",
    "    else:\n",
    "        prompt = generate_prompt(instruction)\n",
    "\n",
    "    return prompt, output_text\n",
    "\n",
    "# --------------- C) 加載數據集 ---------------\n",
    "def load_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    prompt, answer = format_prompt_and_labels(example)\n",
    "    full_text = prompt + answer\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        full_text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    prompt_len = len(tokenizer(prompt, truncation=True, max_length=MAX_LENGTH)[\"input_ids\"])\n",
    "    labels = encoding[\"input_ids\"].copy()\n",
    "\n",
    "    # Mask Prompt 部分的 Label\n",
    "    for i in range(prompt_len):\n",
    "        if i < MAX_LENGTH:\n",
    "            labels[i] = -100\n",
    "\n",
    "    encoding[\"labels\"] = labels\n",
    "    return encoding\n",
    "\n",
    "# --------------- D) 模型加載和微調 ---------------\n",
    "print(\"Loading base model and tokenizer...\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 準備數據集\n",
    "train_data_list = load_data(\"train.json\")\n",
    "train_dataset = Dataset.from_list(train_data_list)\n",
    "train_dataset = train_dataset.map(tokenize_fn)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=5,\n",
    "    disable_tqdm=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(\"LoRA fine-tuning is complete! The adapter is saved at:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import re\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "# ---------------------------\n",
    "# 1) 載入基礎模型\n",
    "# ---------------------------\n",
    "base_model_name = \"minlik/chinese-alpaca-pro-33b-merged\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,  # 使用 float16 提升性能\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def generate_prompt(instruction, input_text=\"\"):\n",
    "    \"\"\"\n",
    "    生成 Prompt，適配 Chinese-Alpaca 模型\n",
    "    \"\"\"\n",
    "    return f\"\"\"以下是一個問題，請逐步分析並提供每個選項的比較，然後得出最符合邏輯的答案。請在最後明確給出 \"最終選擇：\" 和數字答案。\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "def ask(instruction, input_text=\"\", max_new_tokens=64):\n",
    "    \"\"\"\n",
    "    使用模型生成回答\n",
    "    \"\"\"\n",
    "    prompt = generate_prompt(instruction, input_text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    gen_config = GenerationConfig(\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    raw_answer = output.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    # 提取最終選擇的數字\n",
    "    final_answer_match = re.search(r\"最終選擇：([1234])\", raw_answer)\n",
    "    if final_answer_match:\n",
    "        final_answer = final_answer_match.group(1)\n",
    "    else:\n",
    "        final_answer = \"1\"  # 如果未匹配到答案，設置默認值\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "# ---------------------------\n",
    "# 2) 讀取測試集，進行推理\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_excel(\"AI1000.xlsx\")  # 測試集文件\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        q_id = row[\"題號\"]\n",
    "        article = str(row[\"文章\"]).strip()\n",
    "        question = str(row[\"問題\"]).strip()\n",
    "        option1 = str(row[\"選項1\"]).strip()\n",
    "        option2 = str(row[\"選項2\"]).strip()\n",
    "        option3 = str(row[\"選項3\"]).strip()\n",
    "        option4 = str(row[\"選項4\"]).strip()\n",
    "\n",
    "        # 組合 input_text\n",
    "        input_text = (\n",
    "            f\"【文章】{article}\\n\"\n",
    "            f\"【問題】{question}\\n\"\n",
    "            f\"【選項】\\n1) {option1}\\n2) {option2}\\n3) {option3}\\n4) {option4}\"\n",
    "        )\n",
    "\n",
    "        instruction = (\n",
    "            \"請仔細閱讀以下文章和問題，並比較每個選項的內容。\"\n",
    "            \"根據文章中的信息，逐一排除不符合邏輯的選項，最後選擇最符合條件的正確答案（1、2、3 或 4）。\"\n",
    "            \"請注意，只允許輸出一個數字答案，並避免任何解釋或多餘的字符。\"\n",
    "        )\n",
    "\n",
    "        # 使用模型回答\n",
    "        answers = [ask(instruction, input_text, max_new_tokens=64) for _ in range(3)]\n",
    "        final_answer = max(set(answers), key=answers.count)  # 投票選出最多的答案\n",
    "\n",
    "        print(f\"ID={q_id}, Answer={final_answer}\")\n",
    "        results.append([q_id, final_answer])\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3) 寫入 CSV 文件\n",
    "    # ---------------------------\n",
    "    with open(\"Kaggle-sample.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
    "        writer = csv.writer(fw)\n",
    "        writer.writerow([\"ID\", \"Answer\"])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(\"推理完成，答案已寫入 Kaggle-sample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
