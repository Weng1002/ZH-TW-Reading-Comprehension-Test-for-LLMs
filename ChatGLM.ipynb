{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas openpyxl\n",
    "!pip install bitsandbytes==0.38.1\n",
    "!pip install torch transformers==4.31.0 peft==0.4.0 sentencepiece bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换 Llama 的相关模块为 ChatGLM2\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "\n",
    "# 超参数\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 2\n",
    "LR = 1e-4\n",
    "MAX_LENGTH = 1024\n",
    "OUTPUT_DIR = \"ChatGLM2-lora-out\"\n",
    "TRAIN_FILE = \"train.json\"\n",
    "PHASE_SIZE = 100\n",
    "\n",
    "# 加载 ChatGLM2 模型和 tokenizer\n",
    "base_model_name = \"THUDM/chatglm2-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],  # ChatGLM2 支持的 LoRA 模块\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 数据处理函数\n",
    "def load_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def format_prompt_and_labels(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output_text = example[\"output\"]\n",
    "    if input_text.strip():\n",
    "        prompt = f\"\"\"問題：{instruction}\\n\\n{input_text}\\n\\n答：\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"問題：{instruction}\\n\\n答：\"\"\"\n",
    "    return prompt, output_text\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    prompt, answer = format_prompt_and_labels(example)\n",
    "    full_text = prompt + answer\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt, truncation=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    tokenized_answer = tokenizer(answer, truncation=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    dynamic_max_length = min(MAX_LENGTH, len(tokenized_prompt[0]) + len(tokenized_answer[0]) + 10)\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        full_text,\n",
    "        max_length=dynamic_max_length,  \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",          \n",
    "    )\n",
    "\n",
    "    prompt_len = len(tokenized_prompt[0])\n",
    "    labels = encoding[\"input_ids\"].copy()\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "    # print(f\"有效 labels 数量: {sum(1 for label in labels if label != -100)}\")\n",
    "    \n",
    "    return {\"input_ids\": encoding[\"input_ids\"], \"labels\": labels}\n",
    "\n",
    "train_data_list = load_data(TRAIN_FILE)\n",
    "num_phases = (len(train_data_list) + PHASE_SIZE - 1) // PHASE_SIZE\n",
    "\n",
    "train_data_list = load_data(TRAIN_FILE)\n",
    "train_dataset = Dataset.from_list(train_data_list)\n",
    "train_dataset = train_dataset.map(tokenize_fn)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,    \n",
    "    gradient_accumulation_steps=8,     \n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    bf16=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    disable_tqdm=False,                 # 顯示tqdm\n",
    "    logging_strategy=\"steps\",           # 每steps都log\n",
    "    report_to=[],                       # 不用wandb\n",
    "    gradient_checkpointing=False,       # 不用梯度检查点\n",
    ")\n",
    "\n",
    "# --------------- G) 建立 Trainer 並微調 ---------------\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\",  \n",
    "    pad_to_multiple_of=8,  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,  \n",
    ")\n",
    "\n",
    "for step, _ in enumerate(trainer.get_train_dataloader()):\n",
    "    if step % 100 == 0:  # 定期清理缓存\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "for phase in range(num_phases):\n",
    "    print(f\"Phase {phase + 1}/{num_phases}\")\n",
    "    partial_train_data = train_data_list[phase * PHASE_SIZE:(phase + 1) * PHASE_SIZE]\n",
    "    partial_dataset = Dataset.from_list(partial_train_data)\n",
    "    partial_dataset = partial_dataset.map(tokenize_fn)\n",
    "\n",
    "    trainer.train_dataset = partial_dataset  # 更新当前阶段的数据集\n",
    "    trainer.train()\n",
    "\n",
    "    # 定期释放 GPU 内存\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(\"ChatGLM2 LoRA fine-tuning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ---------------------------\n",
    "# 1) 載入 ChatGLM2 模型\n",
    "# ---------------------------\n",
    "base_model_name = \"THUDM/chatglm2-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(base_model_name, trust_remote_code=True).half().cuda()\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Prompt 和推理\n",
    "# ---------------------------\n",
    "def generate_prompt(instruction, input_text=\"\"):\n",
    "    \"\"\"\n",
    "    設計 Chain-of-Thought (CoT) 推理提示，要求模型逐步分析選項並選擇答案。\n",
    "    \"\"\"\n",
    "    return f\"\"\"以下是一段文章和一個問題，請逐步分析文章和每個選項的內容，並選出最符合邏輯的正確答案。\n",
    "    請按照以下步驟進行：\n",
    "    1. 閱讀文章，理解核心內容。\n",
    "    2. 分析問題的含義，逐一比較選項。\n",
    "    3. 排除不符合條件的選項。\n",
    "    4. 清晰地給出 \"最終答案：\" 並僅返回一個數字選項 (1, 2, 3, 或 4)。\n",
    "\n",
    "    ### Instruction:\n",
    "    {instruction}\n",
    "\n",
    "    ### Input:\n",
    "    {input_text}\n",
    "\n",
    "    ### Response:\"\"\"\n",
    "\n",
    "def ask(instruction, input_text=\"\", max_new_tokens=64):\n",
    "    \"\"\"\n",
    "    使用 ChatGLM2 模型生成回答，並提取最終答案。\n",
    "    \"\"\"\n",
    "    prompt = generate_prompt(instruction, input_text)\n",
    "    response, _ = model.chat(tokenizer, prompt, max_length=max_new_tokens, temperature=0.8, top_p=0.85)\n",
    "\n",
    "    # 提取 \"最終答案\" 或第一個合法數字\n",
    "    match = re.search(r\"最終答案：([1234])\", response)\n",
    "    if match:\n",
    "        final_answer = match.group(1)\n",
    "    else:\n",
    "        # 如果沒有找到，則取第一個合法的 1, 2, 3, 4\n",
    "        numbers = re.findall(r\"[1234]\", response)\n",
    "        final_answer = numbers[0] if numbers else \"1\"  # 默認答案為 1\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "# ---------------------------\n",
    "# 3) 讀取測試集並推理\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_excel(\"AI1000.xlsx\")  # 測試集\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        q_id = row[\"題號\"]\n",
    "        article = str(row[\"文章\"]).strip()\n",
    "        question = str(row[\"問題\"]).strip()\n",
    "        option1 = str(row[\"選項1\"]).strip()\n",
    "        option2 = str(row[\"選項2\"]).strip()\n",
    "        option3 = str(row[\"選項3\"]).strip()\n",
    "        option4 = str(row[\"選項4\"]).strip()\n",
    "\n",
    "        # 組合成 input_text\n",
    "        input_text = (\n",
    "            f\"【文章】{article}\\n\"\n",
    "            f\"【問題】{question}\\n\"\n",
    "            f\"【選項】\\n1) {option1}\\n2) {option2}\\n3) {option3}\\n4) {option4}\"\n",
    "        )\n",
    "\n",
    "        instruction = (\n",
    "            \"請仔細閱讀以下文章和問題，並比較每個選項的內容。\"\n",
    "            \"根據文章中的信息，逐一排除不符合邏輯的選項，最後選擇最符合條件的正確答案（1、2、3 或 4）。\"\n",
    "            \"請注意，只允許輸出一個數字答案，並避免任何解釋或多餘的字符。\"\n",
    "        )\n",
    "\n",
    "        # 取得模型回答\n",
    "        answers = [ask(instruction, input_text, max_new_tokens=128) for _ in range(3)]\n",
    "        final_answer = max(set(answers), key=answers.count)  # 投票選出最多的答案\n",
    "\n",
    "        print(f\"ID={q_id}, Answer={final_answer}\")\n",
    "        results.append([q_id, final_answer])\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4) 寫入 CSV 檔\n",
    "    # ---------------------------\n",
    "    with open(\"Kaggle-sample.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as fw:\n",
    "        writer = csv.writer(fw)\n",
    "        writer.writerow([\"ID\", \"Answer\"])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(\"推理完成，答案已寫入 Kaggle-sample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
